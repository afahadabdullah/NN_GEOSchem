{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf4e2517",
   "metadata": {},
   "source": [
    "# GEOS-CF Neural Network Training Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ceffa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import argparse\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data_path = Path(\"../\")\n",
    "output_path = Path(\"../output/\")\n",
    "\n",
    "\n",
    "# conda activate mlGpu2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c55c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_model(nn.Module):\n",
    "    def __init__(self, in_shape=None, out_shape=None):\n",
    "        super(FC_model, self).__init__()\n",
    "        \n",
    "        feat_in = in_shape[1]\n",
    "        feat_out = out_shape[1]\n",
    "        \n",
    "        self.fc1 = nn.Linear(feat_in, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, feat_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "        #print(f\"Input shape: {x.shape}\") \n",
    "        x = F.relu(self.fc1(x))\n",
    "        #print(f\"FC1 shape: {x.shape}\")\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #print(f\"FC2 shape: {x.shape}\")\n",
    "        x = self.fc3(x)\n",
    "        #print(f\"FC3 shape: {x.shape}\")\n",
    "        \n",
    "        return(x)\n",
    "\n",
    "\n",
    "# iteratively train the model (args, Net(), cpu/gpu, how to load trining data, optimizer, number of epochs/iterations) - first epoch index is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee33f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_loader, optimizer, epoch, loss_fxn, loss_dict):\n",
    "    # set the model in training mode: Docs>torch.nn>Module. Training mode affects some modules differently than eval() (used for validation/testing), i.e. Dropout, where no channels/nodes would be zeroed out during testing.\n",
    "    model.train()\n",
    "    # iterate over minibatches\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send I/O data to cpu/gpu\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()  # clear gradients for all tensors\n",
    "        output = model(data)  # calculate yhat\n",
    "        # compute loss between (yhat,y).\n",
    "        loss = loss_fxn(output, target)\n",
    "        loss.backward()  # perform back prop \n",
    "        optimizer.step()  # update parameters\n",
    "        # print training progress to console if there is no remainder in the division of batch_idx/args.log_interval\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            if args.dry_run:\n",
    "                break\n",
    "                \n",
    "    loss_dict['train_loss'].append(float(loss))\n",
    "\n",
    "\n",
    "# validate the trained model on the val set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375a3e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(model, device, val_loader, loss_fxn, loss_dict):\n",
    "    model.eval()  # enter evaluation mode (no Dropout, batchnorm, others?)\n",
    "    # initialize vars\n",
    "    val_loss = 0  \n",
    "    # the following syntax will run the model without tracking computations on each tensor - saves memory\n",
    "    with torch.no_grad():\n",
    "        # val one each sample/label pair in val set\n",
    "        for data, target in val_loader:\n",
    "            # send to cpu/gpu device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            val_feat = data.shape[1]\n",
    "            output = model(data)#, in_channels=val_feat, out_channels=val_feat)  # compute yhat\n",
    "            val_loss += float(F.mse_loss(output, target, reduction='sum').item())  # sum up batch loss\n",
    "\n",
    "    # compute mean of val_loss that was iteratively summed in val loop over samples\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    loss_dict['val_loss'].append(float(val_loss))\n",
    "\n",
    "    # display results on val set to console\n",
    "    print('\\nValidation set: Average loss: {:.6f}\\n'.format(val_loss))\n",
    "\n",
    "# save a checkpoint at the last epoch with minimum val loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "108f415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filename=str(output_path / 'GEOS-CF_checkpoint.pth.tar')):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, str(output_path / 'GEOS-CF_best.pth.tar'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eddaa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_x, train_y, val_x, val_y, args, kwargs):\n",
    "    \n",
    " \n",
    "    # prep iterable datasets for train/val\n",
    "    print(\"####PREPARE TENSOR DATASETS\")\n",
    "    data1 = torch.utils.data.TensorDataset(train_x,train_y)\n",
    "    data2 = torch.utils.data.TensorDataset(val_x,val_y)\n",
    "    \n",
    "    # load train/val data given keyword arguments\n",
    "    train_loader = torch.utils.data.DataLoader(data1,**kwargs)\n",
    "    kwargs.update({'batch_size': args.val_batch_size})\n",
    "    val_loader = torch.utils.data.DataLoader(data2, **kwargs)\n",
    "    \n",
    "    # define model\n",
    "    model = FC_model(in_shape=train_x.shape, out_shape=(0,1)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "                     \n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:  \n",
    "        if os.path.isfile(args.checkpoint):\n",
    "            print(\"=> loading best checkpoint '{}'\".format(args.checkpoint))\n",
    "            checkpoint = torch.load(args.checkpoint)\n",
    "            args.start_epoch = checkpoint['epoch']\n",
    "            min_val_loss = checkpoint['min_val_loss']\n",
    "            GEOSFP2ERA5.load_state_dict(checkpoint['state_dict'])\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "                  .format(args.resume, checkpoint['epoch']))\n",
    "        else:\n",
    "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n",
    "    else:\n",
    "        print(\"=> starting training from scratch ... \")\n",
    "\n",
    "    # define a dict to store loss during training\n",
    "    loss_dict = {'epoch':[],'train_loss':[],'val_loss':[]}\n",
    "    \n",
    "    # start training timer\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"\\n****** TRAIN/VAL ****** \\n\")\n",
    "    # for each epoch, train on training data, val trained model on val data\n",
    "    for epoch in range(args.start_epoch, args.epochs + 1):\n",
    "        loss_dict['epoch'].append(epoch)\n",
    "        train(args, model, device, train_loader, optimizer, epoch, criterion, loss_dict)\n",
    "        val(model, device, val_loader, criterion, loss_dict)\n",
    "        \n",
    "        # timer for training\n",
    "        end_time = time.time() - start_time\n",
    "        print('TRAINING TIME: ',end_time/60,' MINUTES')\n",
    "        \n",
    "        tmp = {\n",
    "        'epoch': epoch,\n",
    "        'train_mse': loss_dict['train_loss'][-1],\n",
    "        'val_mse:': loss_dict['val_loss'][-1],\n",
    "        'runtime': end_time/60\n",
    "        }\n",
    "        \n",
    "        # write loss to file during training to monitor\n",
    "        tmp_df = pd.DataFrame(tmp,index=[0])\n",
    "        if not os.path.exists('monitor_loss.csv'):\n",
    "            tmp_df.to_csv('monitor_loss.csv', mode='w+', index=False, header=True)\n",
    "        else:\n",
    "            tmp_df.to_csv('monitor_loss.csv', mode='a', index=False, header=False)\n",
    "        \n",
    "        # save checkpoint if the val loss is the min over time\n",
    "        val_loss = loss_dict['val_loss']\n",
    "        min_val_loss = np.nanmin(val_loss)\n",
    "        is_best = min_val_loss <= val_loss[-1]\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'min_val_loss': min_val_loss,\n",
    "            'optimizer' : optimizer.state_dict()\n",
    "        }, is_best)\n",
    "        \n",
    "        # if val loss has not improved within the last N epochs by delta, then exit the training loop\n",
    "        # do this by comparing the mean of the last N epochs to the loss of the current epoch\n",
    "        stop_intvl = 5\n",
    "        delta = 0.0001\n",
    "        if len(val_loss) <= stop_intvl:\n",
    "            continue\n",
    "        lastN = val_loss[-stop_intvl-1:-1]\n",
    "        diff = np.abs(np.mean(lastN) - val_loss[-1])\n",
    "        if diff <= delta:\n",
    "            print('EARLY STOPPING - validation loss has not improved by less than '+ str(delta) +' in '+ str(stop_intvl) +' epochs')\n",
    "            break\n",
    "        \n",
    "    # write loss dict to csv\n",
    "    loss_df = pd.DataFrame(loss_dict)\n",
    "    if not os.path.exists(str(output_path / 'loss.csv')):\n",
    "        loss_df.to_csv('loss.csv', mode='w+', index=False, header=True)\n",
    "    else:\n",
    "        loss_df.to_csv('loss.csv', mode='a', index=False, header=False)\n",
    "                \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f70dfcbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(fls=[], test=False):\n",
    "    # 0) load & pick\n",
    "    df = pd.concat([pd.read_parquet(f) for f in fls], ignore_index=True)\n",
    "    feat_list   = [\"Jval\", \"Met\", \"ConcBeforeChem\", \"AeroArea\"]\n",
    "    pattern     = \"|\".join(feat_list)\n",
    "    features_df = df.filter(regex=pattern)      # (N,66)\n",
    "    targets_df  = df[\"ConcAfterChem_CO\"]        # (N,)\n",
    "\n",
    "    if test:\n",
    "        X = torch.tensor(features_df.values, dtype=torch.float32)\n",
    "        y = torch.tensor(targets_df.values,  dtype=torch.float32).unsqueeze(1)\n",
    "        return X, y\n",
    "\n",
    "    # 1) split\n",
    "    X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(\n",
    "        features_df, targets_df, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # 2) scale X\n",
    "    scalerX     = MinMaxScaler()\n",
    "    X_train_arr = scalerX.fit_transform(X_train_df)\n",
    "    X_val_arr   = scalerX.transform(  X_val_df  )\n",
    "\n",
    "    # 3) scale Y (reshape to 2D!)\n",
    "    scalerY      = MinMaxScaler()\n",
    "    y_train_arr  = scalerY.fit_transform(y_train_df.values.reshape(-1,1))\n",
    "    y_val_arr    = scalerY.transform(  y_val_df.values.reshape(-1,1))\n",
    "\n",
    "    # sanity‐check:\n",
    "    print(\"Y train min/max:\", y_train_arr.min(), y_train_arr.max())  # → 0.0, 1.0\n",
    "\n",
    "    # 4) to torch (arrays are already shape (n,1))\n",
    "    X_train = torch.tensor(X_train_arr, dtype=torch.float32)\n",
    "    X_val   = torch.tensor(X_val_arr,   dtype=torch.float32)\n",
    "    y_train = torch.tensor(y_train_arr, dtype=torch.float32)\n",
    "    y_val   = torch.tensor(y_val_arr,   dtype=torch.float32)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "\n",
    "\n",
    "train_model = True\n",
    "test_model = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e0f374f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--batch-size N] [--val-batch-size N]\n",
      "                             [--epochs N] [--start-epoch N] [--lr LR]\n",
      "                             [--no-cuda] [--dry-run] [--seed S]\n",
      "                             [--log-interval N] [--checkpoint PATH]\n",
      "                             [--resume RESUME]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /gpfsm/dhome/afahad/.local/share/jupyter/runtime/kernel-95e4ed51-6cf2-4178-b776-114162899de2.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/other/anaconda/GEOSpyD/23.5.2-0_py3.11/2024-01-30/envs/mlGpu2/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if train_model:\n",
    "    # Training settings\n",
    "    # argparse is a python module...I don't fully understand it, but this is how the hyperparameters are specified\n",
    "    parser = argparse.ArgumentParser(description='')\n",
    "    parser.add_argument('--batch-size', type=int, default=3200, metavar='N',\n",
    "                        help='input batch size for training')  # if the batch size is larger than 1, we get an OOM at the first conv\n",
    "    parser.add_argument('--val-batch-size', type=int, default=3200, metavar='N',\n",
    "                        help='input batch size for testing/validation')\n",
    "    parser.add_argument('--epochs', type=int, default=20, metavar='N',\n",
    "                        help='number of epochs to train ')\n",
    "    parser.add_argument('--start-epoch', default=0, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "    parser.add_argument('--lr', type=float, default=0.001, metavar='LR',\n",
    "                        help='learning rate ')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--dry-run', action='store_true', default=True,\n",
    "                        help='quickly check a single pass')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=1, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--checkpoint', default=str(output_path / 'GEOS-CF_checkpoint.pth.tar'), type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "    parser.add_argument('--resume', default=False, \n",
    "                    help='resume training from last checkpoint')\n",
    "    args = parser.parse_args()  # input arguments\n",
    "   \n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()  # use GPU or not\n",
    "\n",
    "    torch.manual_seed(args.seed)  # set the random seed\n",
    "\n",
    "    # device = torch.device(\"cuda\" if use_cuda else \"cpu\")  # define gpu/cpu device\n",
    "    device = torch.device(\"cuda\")  # define gpu/cpu device\n",
    "\n",
    "    # *args are non-keyword arguments, *kwargs are keyword arguments\n",
    "    kwargs = {'batch_size': args.batch_size}\n",
    "    # GPU settings\n",
    "    if use_cuda:\n",
    "        kwargs.update({'num_workers': 1,\n",
    "                       'pin_memory': True,\n",
    "                       'shuffle': True},\n",
    "                     )\n",
    "\n",
    "    \n",
    "    # read files specified for training\n",
    "    with open(\"training_filenames.txt\", \"r\") as f:\n",
    "        train_fls = [line.strip() for line in f]\n",
    "    #print(train_fls)\n",
    "    #train_fls=train_fls[0:1]\n",
    "        \n",
    "    train_X, train_Y, val_X, val_Y = get_data(fls=train_fls, test=False)\n",
    "    \n",
    "    #print(train_X.shape, val_X.shape)\n",
    "\n",
    "    for t in (train_X, train_Y, val_X, val_Y): \n",
    "        t[torch.isnan(t)] = -0.99\n",
    "    \n",
    "    # prepare dataloader, train, and validate\n",
    "    main(train_X, train_Y, val_X, val_Y, args, kwargs)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e03bb-1390-4089-af27-79d2987e9201",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a251f-4373-494d-9921-7cdfc3b8eae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed9e96-d454-4f6c-b73a-991f8b6830dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0193ea94-8ff4-4799-a910-4276e8a8589e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlGPU2)",
   "language": "python",
   "name": "mlgpu2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
